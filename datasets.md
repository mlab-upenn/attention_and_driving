---
<a href=README.md/#top><l style="font-size:30px">Home</l></a>&nbsp;&nbsp;| <a href=behavioral.md><l style="font-size:30px">Behavioral</l></a>&nbsp;&nbsp;| <a href=scene_gaze.md><l style="font-size:30px">Applications</l></a>&nbsp;&nbsp;| <l style="font-size:35px">Datasets</l>&nbsp;&nbsp;
---

*Click on each entry below to see additional information.*
<a name=TrafficSaliency></a>
<details close>
<summary>TrafficSaliency | <a href=https://doi.org/10.1109/TITS.2019.2915540>paper</a> | <a href=https://github.com/taodeng/CDNN-traffic-saliency>link</a></summary>
<ul>
Description: 16 videos of driving scenes with gaze data of 28 subjects recorded in the lab with eye-tracker
</ul>
</summary>
<ul>
Data: eye-tracking, scene video
</ul>
<ul>
<pre>
@article{2020_T-ITS_Deng,
    author = "Deng, Tao and Yan, Hongmei and Qin, Long and Ngo, Thuyen and Manjunath, BS",
    title = "How do drivers allocate their potential attention? Driving fixation prediction via convolutional neural networks",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    volume = "21",
    number = "5",
    pages = "2146--2154",
    year = "2019",
    publisher = "IEEE"
}
</pre>
</details>
</ul>

<a name=DADA-2000></a>
<details close>
<summary>DADA-2000 | <a href=https://doi.org/10.1109/ITSC.2019.8917218>paper</a> | <a href=https://github.com/JWFangit/LOTVS-DADA>link</a></summary>
<ul>
Description: 2000 videos of accident videos collected from video hosting websites with eye-tracking data from 20 subjects collected in the lab.
</ul>
</summary>
<ul>
Data: eye-tracking, scene video
</ul>
</summary>
<ul>
Annotations: bounding boxes, accident category labels
</ul>
<ul>
<pre>
@inproceedings{2019_ITSC_Fang,
    author = "Fang, Jianwu and Yan, Dingxin and Qiao, Jiahuan and Xue, Jianru and Wang, He and Li, Sen",
    title = "{DADA-2000: Can Driving Accident be Predicted by Driver Attentionƒ Analyzed by A Benchmark}",
    booktitle = "ITSC",
    year = "2019"
}
</pre>
</details>
</ul>

<a name=PRORETA 4></a>
<details close>
<summary>PRORETA 4 | <a href=https://doi.org/10.1109/IVS.2019.8814224>paper</a> | <a href=https://www.proreta.tu-darmstadt.de/proreta_1_4/proreta4_1/datasets_1/index.en.jsp>link</a></summary>
<ul>
Description: Videos of traffic scenes recorded in instrumented vehicle with driver’s gaze data for evaluating accuracy of detecting driver’s current object of fixation
</ul>
</summary>
<ul>
Data: eye-tracking, driver video, scene video
</ul>
<ul>
<pre>
@inproceedings{2019_IV_Schwehr,
    author = "Schwehr, Julian and Knaust, Moritz and Willert, Volker",
    title = "How to evaluate object-of-fixation detection",
    booktitle = "IV",
    year = "2019"
}
</pre>
</details>
</ul>

<a name=DR(eye)VE></a>
<details close>
<summary>DR(eye)VE | <a href=https://doi.org/10.1109/TPAMI.2018.2845370>paper</a> | <a href=http://imagelab.ing.unimore.it/dreyeve>link</a></summary>
<ul>
Description: Driving videos recorded on-road with corresponding gaze data of the driver
</ul>
</summary>
<ul>
Data: eye-tracking, scene video, vehicle data
</ul>
</summary>
<ul>
Annotations: weather and road type labels
</ul>
<ul>
<pre>
@article{2018_PAMI_Palazzi,
    author = "Palazzi, Andrea and Abati, Davide and Solera, Francesco and Cucchiara, Rita and others",
    title = "{Predicting the Driver's Focus of Attention: the DR (eye) VE Project}",
    journal = "IEEE TPAMI",
    volume = "41",
    number = "7",
    pages = "1720--1733",
    year = "2018"
}
</pre>
</details>
</ul>

<a name=BBD-A></a>
<details close>
<summary>BBD-A | <a href=https://doi.org/10.1007/978-3-030-20873-8_42>paper</a> | <a href=https://bdd-data.berkeley.edu/>link</a></summary>
<ul>
Description: A set of short video clips extracted from the Berkeley Deep Drive (BDD) dataset with additional eye-tracking data collected in the lab from 45 subjects
</ul>
</summary>
<ul>
Data: eye-tracking, scene video, vehicle data
</ul>
<ul>
<pre>
@inproceedings{2018_ACCV_Xia,
    author = "Xia, Ye and Zhang, Danqing and Kim, Jinkyu and Nakayama, Ken and Zipser, Karl and Whitney, David",
    title = "Predicting driver attention in critical situations",
    booktitle = "ACCV",
    year = "2018"
}
</pre>
</details>
</ul>

<a name=C42CN></a>
<details close>
<summary>C42CN | <a href=https://doi.org/10.1038/sdata.2017.110>paper</a> | <a href=https://osf.io/c42cn/>link</a></summary>
<ul>
Description: A multi-modal dataset acquired in a controlled experiment on a driving simulator under 4 conditions: no distraction, cognitive, emotional and sensorimotor distraction.
</ul>
</summary>
<ul>
Data: eye-tracking, scene video, physiological signal
</ul>
<ul>
<pre>
@article{2017_NatSciData_Taamneh,
    author = "Taamneh, Salah and Tsiamyrtzis, Panagiotis and Dcosta, Malcolm and Buddharaju, Pradeep and Khatri, Ashik and Manser, Michael and Ferris, Thomas and Wunderlich, Robert and Pavlidis, Ioannis",
    title = "A multimodal dataset for various forms of distracted driving",
    journal = "Scientific Data",
    volume = "4",
    pages = "170110",
    year = "2017"
}
</pre>
</details>
</ul>

<a name=TETD></a>
<details close>
<summary>TETD | <a href=https://doi.org/10.1109/TITS.2016.2535402>paper</a> | <a href=https://github.com/taodeng/traffic-eye-tracking-dataset>link</a></summary>
<ul>
Description: A set of 100 images of traffic scenes with corresponding eye-tracking data from 20 subjects
</ul>
</summary>
<ul>
Data: eye-tracking, scene images
</ul>
<ul>
<pre>
@article{2016_T-ITS_Deng,
    author = "Deng, Tao and Yang, Kaifu and Li, Yongjie and Yan, Hongmei",
    title = "Where does the driver look? Top-down-based saliency detection in a traffic driving environment",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    volume = "17",
    number = "7",
    pages = "2051--2062",
    year = "2016",
    publisher = "IEEE"
}
</pre>
</details>
</ul>

<a name=3DDS></a>
<details close>
<summary>3DDS | <a href=http://www.bmva.org/bmvc/2011/proceedings/paper85/paper85.pdf>paper</a> | <a href=http://ilab.usc.edu/borji/Resources.html>link</a></summary>
<ul>
Description: Videos and eye-tracking data of people playing 3D driving simulator game
</ul>
</summary>
<ul>
Data: eye-tracking, scene video
</ul>
<ul>
<pre>
@inproceedings{2011_BMVC_Borji,
    author = "Borji, Ali and Sihite, Dicky N and Itti, Laurent",
    title = "Computational Modeling of Top-down Visual Attention in Interactive Environments.",
    booktitle = "BMVC",
    year = "2011"
}
</pre>
</details>
</ul>

<a name=VADD></a>
<details close>
<summary>VADD | <a href=https://doi.org/10.1109/TITS.2021.3053178>paper</a> | <a href=https://github.com/epan-utbm/VADD-Saliency-Dataset>link</a></summary>
<ul>
Description: not available
</ul>
<ul>
<pre>
@article{2021_T-ITS_Lateef,
    author = "Lateef, Fahad and Kas, Mohamed and Ruichek, Yassine",
    title = "Saliency heat-map as visual attention for autonomous driving using generative adversarial network (gan)",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    year = "2021"
}
</pre>
</details>
</ul>

<a name=LISA v2></a>
<details close>
<summary>LISA v2 | <a href=https://doi.org/10.1109/IV47402.2020.9304573>paper</a> | <a href=https://github.com/arangesh/GPCycleGAN>link</a></summary>
<ul>
Description: Videos of drivers with and without eyeglasses recorded under different lighting conditions
</ul>
</summary>
<ul>
Data: driver video
</ul>
<ul>
<pre>
@inproceedings{2020_IV_Rangesh,
    author = "Rangesh, Akshay and Zhang, Bowen and Trivedi, Mohan M",
    title = "Driver gaze estimation in the real world: Overcoming the eyeglass challenge",
    booktitle = "IV",
    year = "2020"
}
</pre>
</details>
</ul>

<a name=DGAZE></a>
<details close>
<summary>DGAZE | <a href=http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/ConferencePapers/2020/DGAZE_Driver.pdf>paper</a> | <a href=https://github.com/duaisha/DGAZE>link</a></summary>
<ul>
Description: A dataset mapping drivers’ gaze to different areas in a static traffic scene in lab conditions
</ul>
</summary>
<ul>
Data: driver video, scene video
</ul>
</summary>
<ul>
Annotations: bounding boxes
</ul>
<ul>
<pre>
@inproceedings{2020_IROS_Dua,
    author = "Dua, Isha and John, Thrupthi Ann and Gupta, Riya and Jawahar, CV",
    title = "DGAZE: Driver Gaze Mapping on Road",
    booktitle = "IROS",
    year = "2020"
}
</pre>
</details>
</ul>

<a name=DGW></a>
<details close>
<summary>DGW | <a href=https://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Ghosh_Speak2Label_Using_Domain_Knowledge_for_Creating_a_Large_Scale_Driver_ICCVW_2021_paper.pdf>paper</a> | <a href=https://sites.google.com/view/drivergazeprediction/home>link</a></summary>
<ul>
Description: Videos of drivers fixating on different areas in the vehicle without constraining their head and eye movements
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: gaze area labels
</ul>
<ul>
<pre>
@inproceedings{2021_ICCVW_Ghosh,
    author = "Ghosh, Shreya and Dhall, Abhinav and Sharma, Garima and Gupta, Sarthak and Sebe, Nicu",
    title = "Speak2label: Using domain knowledge for creating a large scale driver gaze zone estimation dataset",
    booktitle = "ICCVW",
    year = "2021"
}
</pre>
</details>
</ul>

<a name=DMD></a>
<details close>
<summary>DMD | <a href=https://doi.org/10.1007/978-3-030-66823-5_23>paper</a> | <a href=https://dmd.vicomtech.org/>link</a></summary>
<ul>
Description: A diverse multi-modal dataset of drivers performing various secondary tasks, observing different regions inside the car, and showing signs of drowsiness recorded on-road and in simulation environment
</ul>
</summary>
<ul>
Data: driver video, scene video, vehicle data
</ul>
</summary>
<ul>
Annotations: bounding boxes, action labels
</ul>
<ul>
<pre>
@inproceedings{2020_ECCVW_Ortega,
    author = "Ortega, Juan Diego and Kose, Neslihan and Ca{\\textasciitilde n}as, Paola and Chao, Min-An and Unnervik, Alexander and Nieto, Marcos and Otaegui, Oihana and Salgado, Luis",
    title = "Dmd: A large-scale multi-modal driver monitoring dataset for attention and alertness analysis",
    booktitle = "ECCV",
    year = "2020"
}
</pre>
</details>
</ul>

<a name=NeuroIV></a>
<details close>
<summary>NeuroIV | <a href=https://doi.org/10.1109/TITS.2020.3022921>paper</a> | <a href=https://github.com/ispc-lab/NeuroIV>link</a></summary>
<ul>
Description: Videos of drivers performing secondary tasks, making hand gestures and observing different regions inside the vehicle recorded with DAVIS and depth sensor
</ul>
</summary>
<ul>
Data: driver video
</ul>
<ul>
<pre>
@article{2020_T-ITS_Chen,
    author = {Chen, Guang and Wang, Fa and Li, Weijun and Hong, Lin and Conradt, J{\"o}rg and Chen, Jieneng and Zhang, Zhenyan and Lu, Yiwen and Knoll, Alois},
    title = "NeuroIV: Neuromorphic vision meets intelligent vehicle towards safe driving with a new database and baseline evaluations",
    journal = "IEEE Transactions on Intelligent Transportation Systems",
    year = "2020"
}
</pre>
</details>
</ul>

<a name=Drive&Act></a>
<details close>
<summary>Drive&Act | <a href=https://openaccess.thecvf.com/content_ICCV_2019/papers/Martin_DriveAct_A_Multi-Modal_Dataset_for_Fine-Grained_Driver_Behavior_Recognition_in_ICCV_2019_paper.pdf>paper</a> | <a href=https://www.driveandact.com/>link</a></summary>
<ul>
Description: Videos of drivers performing various driving- and non-driving-related tasks
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: semantic maps, action labels
</ul>
<ul>
<pre>
@inproceedings{2019_ICCV_Martin,
    author = "Martin, Manuel and Roitberg, Alina and Haurilet, Monica and Horne, Matthias and Rei{\ss}, Simon and Voit, Michael and Stiefelhagen, Rainer",
    title = "Drive\\&act: A multi-modal dataset for fine-grained driver behavior recognition in autonomous vehicles",
    booktitle = "ICCV",
    year = "2019"
}
</pre>
</details>
</ul>

<a name=HAD></a>
<details close>
<summary>HAD | <a href=https://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_Grounding_Human-To-Vehicle_Advice_for_Self-Driving_Vehicles_CVPR_2019_paper.pdf>paper</a> | <a href=https://usa.honda-ri.com/HAD>link</a></summary>
<ul>
Description: A subset of videos from HDD naturalistic dataset annotated with textual advice containing 1) goals – where the vehicle should move and 2) attention – where the vehicle should look
</ul>
</summary>
<ul>
Data: scene video, vehicle data
</ul>
</summary>
<ul>
Annotations: goal and attention labels
</ul>
<ul>
<pre>
@inproceedings{2019_CVPR_Kim,
    author = "Kim, Jinkyu and Misu, Teruhisa and Chen, Yi-Ting and Tawari, Ashish and Canny, John",
    title = "Grounding human-to-vehicle advice for self-driving vehicles",
    booktitle = "CVPR",
    year = "2019"
}
</pre>
</details>
</ul>

<a name=RLDD></a>
<details close>
<summary>RLDD | <a href=https://openaccess.thecvf.com/content_CVPRW_2019/papers/AMFG/Ghoddoosian_A_Realistic_Dataset_and_Baseline_Temporal_Model_for_Early_Drowsiness_CVPRW_2019_paper.pdf>paper</a> | <a href=https://github.com/rezaghoddoosian/Early-Drowsiness-Detection>link</a></summary>
<ul>
Description: Crowdsourced videos of people in various states of drowsiness recorded in indoor environments
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: drowsiness labels
</ul>
<ul>
<pre>
@inproceedings{2019_CVPRW_Ghoddoosian,
    author = "Ghoddoosian, Reza and Galib, Marnim and Athitsos, Vassilis",
    title = "A realistic dataset and baseline temporal model for early drowsiness detection",
    booktitle = "CVPRW",
    year = "2019"
}
</pre>
</details>
</ul>

<a name=BBD-X></a>
<details close>
<summary>BBD-X | <a href=https://openaccess.thecvf.com/content_ECCV_2018/papers/Jinkyu_Kim_Textual_Explanations_for_ECCV_2018_paper.pdf>paper</a> | <a href=https://github.com/JinkyuKimUCB/BDD-X-dataset>link</a></summary>
<ul>
Description: A subset of videos from BDD dataset annotated with textual descriptions of actions performed by the vehicle and explanations justifying those actions
</ul>
</summary>
<ul>
Data: scene video, vehicle data
</ul>
</summary>
<ul>
Annotations: action explanations
</ul>
<ul>
<pre>
@inproceedings{2018_ECCV_Kim,
    author = "Kim, Jinkyu and Rohrbach, Anna and Darrell, Trevor and Canny, John and Akata, Zeynep",
    title = "Textual explanations for self-driving vehicles",
    booktitle = "ECCV",
    year = "2018"
}
</pre>
</details>
</ul>

<a name=HDD></a>
<details close>
<summary>HDD | <a href=https://openaccess.thecvf.com/content_cvpr_2018/papers/Ramanishka_Toward_Driving_Scene_CVPR_2018_paper.pdf>paper</a> | <a href=https://usa.honda-ri.com/HDD>link</a></summary>
<ul>
Description: A large naturalistic driving dataset with driving footage, vehicle telemetry and annotations for vehicle actions and their justifications
</ul>
</summary>
<ul>
Data: scene video, vehicle data
</ul>
</summary>
<ul>
Annotations: bounding boxes, action labels
</ul>
<ul>
<pre>
@inproceedings{2018_CVPR_Ramanishka,
    author = "Ramanishka, Vasili and Chen, Yi-Ting and Misu, Teruhisa and Saenko, Kate",
    title = "Toward driving scene understanding: A dataset for learning driver behavior and causal reasoning",
    booktitle = "CVPR",
    year = "2018"
}
</pre>
</details>
</ul>

<a name=DDD></a>
<details close>
<summary>DDD | <a href=https://doi.org/10.1007/978-3-319-54526-4_9>paper</a> | <a href=http://cv.cs.nthu.edu.tw/php/callforpaper/datasets/DDD/>link</a></summary>
<ul>
Description: Videos of human subjects simulating different levels of drowsiness while driving in a simulator
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: drowsiness labels
</ul>
<ul>
<pre>
@inproceedings{2017_ACCV_Weng,
    author = "Weng, Ching-Hua and Lai, Ying-Hsiu and Lai, Shang-Hong",
    title = "Driver drowsiness detection via a hierarchical temporal deep belief network",
    booktitle = "ACCV",
    year = "2016"
}
</pre>
</details>
</ul>

<a name=DriveAHead></a>
<details close>
<summary>DriveAHead | <a href=https://openaccess.thecvf.com/content_cvpr_2017_workshops/w13/papers/Schwarz_DriveAHead_-_A_CVPR_2017_paper.pdf>paper</a> | <a href=https://cvhci.anthropomatik.kit.edu/data/DriveAHead/>link</a></summary>
<ul>
Description: Videos of drivers with frame-level head pose annotations obtained from a motion-capture system
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: occlusion, head pose, depth
</ul>
<ul>
<pre>
@inproceedings{2017_CVPRW_Schwarz,
    author = "Schwarz, Anke and Haurilet, Monica and Martinez, Manuel and Stiefelhagen, Rainer",
    title = "Driveahead-a large-scale driver head pose dataset",
    booktitle = "CVPRW",
    pages = "1--10",
    year = "2017"
}
</pre>
</details>
</ul>

<a name=DAD></a>
<details close>
<summary>DAD | <a href=https://doi.org/10.1007/978-3-319-54190-7_9>paper</a> | <a href=https://aliensunmin.github.io/project/dashcam/>link</a></summary>
<ul>
Description: Videos of accidents recorded with dashboard cameras sourced from video hosting sites with annotations for accidents and road users involved in them
</ul>
</summary>
<ul>
Data: scene video
</ul>
</summary>
<ul>
Annotations: bounding boxes, accident category labels
</ul>
<ul>
<pre>
@inproceedings{2016_ACCV_Chan,
    author = "Chan, Fu-Hsiang and Chen, Yu-Ting and Xiang, Yu and Sun, Min",
    title = "Anticipating accidents in dashcam videos",
    booktitle = "ACCV",
    year = "2016"
}
</pre>
</details>
</ul>

<a name=Brain4Cars></a>
<details close>
<summary>Brain4Cars | <a href=https://openaccess.thecvf.com/content_iccv_2015/papers/Jain_Car_That_Knows_ICCV_2015_paper.pdf>paper</a> | <a href=https://github.com/asheshjain399/ICCV2015_Brain4Cars>link</a></summary>
<ul>
Description: Synchronized videos from scene and driver-facing cameras of drivers performing various maneuvers in traffic
</ul>
</summary>
<ul>
Data: driver video, scene video, vehicle data
</ul>
</summary>
<ul>
Annotations: action labels
</ul>
<ul>
<pre>
@inproceedings{2015_ICCV_Jain,
    author = "Jain, Ashesh and Koppula, Hema S and Raghavan, Bharad and Soh, Shane and Saxena, Ashutosh",
    title = "Car that knows before you do: Anticipating maneuvers via learning temporal driving models",
    booktitle = "ICCV",
    year = "2015"
}
</pre>
</details>
</ul>

<a name=DROZY></a>
<details close>
<summary>DROZY | <a href=https://doi.org/10.1109/WACV.2016.7477715>paper</a> | <a href=http://www.drozy.ulg.ac.be/>link</a></summary>
<ul>
Description: Videos and physiological data from subjects in different drowsiness states after prolonged waking
</ul>
</summary>
<ul>
Data: driver video, physiological signal
</ul>
</summary>
<ul>
Annotations: drowsiness labels
</ul>
<ul>
<pre>
@inproceedings{2016_WACV_Massoz,
    author = "Massoz, Quentin and Langohr, Thomas and Fran{\c{c}}ois, Cl{\'e}mentine and Verly, Jacques G",
    title = "The ULg multimodality drowsiness database (called DROZY) and examples of use",
    booktitle = "WACV",
    year = "2016"
}
</pre>
</details>
</ul>

<a name=DIPLECS Surrey></a>
<details close>
<summary>DIPLECS Surrey | <a href=https://doi.org/10.1109/TVT.2015.2487826>paper</a> | <a href=https://cvssp.org/data/diplecs/>link</a></summary>
<ul>
Description: Driving videos with steering information recorded in different cars and environments
</ul>
</summary>
<ul>
Data: scene video, vehicle data
</ul>
<ul>
<pre>
@article{2015_TranVehTech_Pugeault,
    author = "Pugeault, Nicolas and Bowden, Richard",
    title = "How much of driving is preattentive?",
    journal = "IEEE Transactions on Vehicular Technology",
    volume = "64",
    number = "12",
    pages = "5424--5438",
    year = "2015",
    publisher = "IEEE"
}
</pre>
</details>
</ul>

<a name=YawDD></a>
<details close>
<summary>YawDD | <a href=https://doi.org/10.1145/2557642.2563678>paper</a> | <a href=https://ieee-dataport.org/open-access/yawdd-yawning-detection-dataset>link</a></summary>
<ul>
Description: Recordings of human subjects in parked vehicles simulating normal driving, singing and taslking, and yawning
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: bounding boxes, action labels
</ul>
<ul>
<pre>
@inproceedings{2014_ACM_Abtahi,
    author = "Abtahi, Shabnam and Omidyeganeh, Mona and Shirmohammadi, Shervin and Hariri, Behnoosh",
    title = "{YawDD: A yawning detection dataset}",
    booktitle = "Proceedings of the ACM Multimedia Systems Conference",
    year = "2014"
}
</pre>
</details>
</ul>

<a name=DIPLECS Sweden></a>
<details close>
<summary>DIPLECS Sweden | <a href=https://doi.org/10.1007/978-3-642-15567-3_12>paper</a> | <a href=https://cvssp.org/data/diplecs/>link</a></summary>
<ul>
Description: Driving videos with steering information recorded in different cars and environments
</ul>
</summary>
<ul>
Data: scene video, vehicle data
</ul>
<ul>
<pre>
@inproceedings{2010_ACCV_Pugeault,
    author = "Pugeault, Nicolas and Bowden, Richard",
    title = "Learning pre-attentive driving behaviour from holistic visual features",
    booktitle = "ECCV",
    year = "2010"
}
</pre>
</details>
</ul>

<a name=Dashcam dataset></a>
<details close>
<summary>Dashcam dataset | <a href=https://github.com/SullyChen/driving-datasets>link</a></summary>
<ul>
Description: Driving videos with steering information recorded on road
</ul>
</summary>
<ul>
Data: scene video
</ul>
<ul>
<pre>
</pre>
</details>
</ul>

<a name=BU HeadTracking></a>
<details close>
<summary>BU HeadTracking | <a href=https://doi.org/10.1109/34.845375>paper</a> | <a href=https://www.cs.bu.edu/groups/ivc/HeadTracking/>link</a></summary>
<ul>
Description: Videos and head tracking information for multiple human subjects recorded in diverse conditions
</ul>
</summary>
<ul>
Data: driver video
</ul>
</summary>
<ul>
Annotations: head pose
</ul>
<ul>
<pre>
@article{2000_PAMI_LaCascia,
    author = "La Cascia, Marco and Sclaroff, Stan and Athitsos, Vassilis",
    title = "Fast, reliable head tracking under varying illumination: An approach based on registration of texture-mapped 3D models",
    journal = "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    volume = "22",
    number = "4",
    pages = "322--336",
    year = "2000"
}
</pre>
</details>
</ul>

